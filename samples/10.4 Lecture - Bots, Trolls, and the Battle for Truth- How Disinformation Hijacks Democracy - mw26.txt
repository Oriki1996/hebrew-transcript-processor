[דובר 1] (0:09 - 5:26)
Hi, imagine two groups of citizens converging on the same street, each fiercely convinced they are defending their community. In May 2016, outside an Islamic center in Houston, Texas, exactly this happened. A protest and counter-protest squared off, hurling slogans and distrust at one another.

What none of the attendees realized was that both events had been orchestrated by the same hidden puppeteer, Russian operatives using fake Facebook accounts. One shunned Facebook page, Heart of Texas, had rallied locals to stop the Islamification of Texas. While another fake persona group, United Muslims of America, simultaneously called for a counter-rally to save Islamic knowledge.

Dozens of Texans showed up, genuinely invested in their causes, and ended up angrily confronting each other. U.S. investigators later revealed that this social conflict was manufactured from afar for as little as $200 in advertising. All by trolls tried to Russia's internet research agency.

A U.S. senator remarked, and I quote, from a computer in St. Petersburg, Russia, these operators can create and promote events anywhere in the United States in an attempt to tear our society apart. That is the reality. This striking incident offers a glimpse into the new era of political manipulation.

Around the world, bots and fake social media personas have become foot soldiers in campaigns to shape public opinion, sow distrust, inflame polarization, and even sway elections. In this talk, we'll explore how political actors, from state-sponsored troll farms to covert private firms, deploy armies of automated accounts and imposter profiles to twist the online conversation. Let us start by looking at how bots shape public opinion.

Social media has become a battleground for hearts and minds. By flooding feeds with certain messages or drowning out others, political bots can skew public opinion. The strategy is often simple, amplify content that helps one side and suppress what hurts it.

In Mexico, where X is a key for activism, protesters face swarms of pro-government bots. Under former president Enrique Peña Nieto, researchers say over 75,000 PeñaBots were used to silence critics by spamming protest hashtags with irrelevant content. When activists used the hashtag ShaMeCanSe to demand answers about 43 missing students, thousands of dummy accounts flooded the tag with gibberish.

Twitter algorithms detecting spam removed the hashtag from trending lists, effectively silencing the protest online. This tactic isn't unique to Mexico. Around the world, authorities and political operatives use bots to control the narrative.

In Turkey, Egypt, and Syria, similar strategies have been noted, with automated accounts swarming inconvenient hashtags used by protesters. In China, state-aligned cyber-troops, often a mix of bots and paid commenters, have flooded social platforms with smear campaigns against Hong Kong protesters, painting them in a negative light. Even in peacetime politics, bot armies create engineered illusions of popularity.

Suddenly a hashtag praising a leader or policy will trend, not because the public is exuberant, but because hundreds of fake accounts are tweeting it in unison. These fake trends can mislead citizens into thinking a viewpoint is more widely supportive than it truly is. In short, by amplifying propaganda and drowning out dissent, bots can act as force multipliers for those in power to manufacture consensus, or at least the appearance of it.

One striking recent revelation is how professionalized this has become. In 2023, an undercover investigation exposed an Israel-based disinformation-for-hire outfit nicknamed Tim Jorge. This private firm offered to covertly manipulate elections and public opinion for a fee, using a sophisticated software called AIMS, Advanced Impact Media Solutions.

AIMS enables a single operator to control an army of more than 30,000 fake online profiles across Twitter, Facebook, Instagram, and more. These aren't ordinary bots spitting out repetitive text. Each fake persona is equipped with a full digital life.

Some even have profile photos generated by AI, LinkedIn accounts, credit cards, and even Airbnb profiles. Let us view together what these Israeli online manipulators did.

[דובר 2] (5:48 - 7:07)
This is Tal Hanan, the mastermind behind Tim Jorge, a covert unit that specializes in hacking and disinformation. For two decades, Hanan's real identity has remained secret. He operates in the shadows using an alias, Jorge.

Now, a joint investigation by The Guardian and Forbidden Stories can reveal who he is, where he's worked, and how he manipulates elections for money. So the disinformation industry is a relatively new phenomenon, which effectively distorts reality and spreads online propaganda. It is usually done by states themselves or by mercenaries for hire who work for private clients or for governments themselves.

Almost always, this is done covertly. To expose this disinformation unit, reporters went undercover as prospective clients who wanted to delay an election in Africa. In six hours of secretly recorded meetings, Hanan and his Israel-based team were filmed demonstrating their services.

[דובר 3] (7:08 - 7:11)
Cyber, special operations, intelligence.

[דובר 2] (7:13 - 7:19)
Hanan has corporate and political clients and boasts about past and present jobs.

[דובר 3] (7:20 - 7:26)
33 presidential-level campaigns we have completed, 27 of which were successful.

[דובר 2] (7:28 - 7:35)
One of Tim Jorge's secret tools is a piece of software called AIMS, which weaponizes social media.

[דובר 3] (7:36 - 7:42)
I'll show you why our system is considered by the type of client that buy it.

[דובר 2] (7:59 - 8:32)
Hanan's team control an army of more than 30,000 bots. A bot is effectively a social media account that pretends to be human but isn't human. It's being controlled through automation.

Tim Jorge's bots or avatars are by far one of the most sophisticated that we've ever seen. The bots are multilayered and have corresponding accounts on several other platforms, much like a human being does, which is why when you look at it initially, it does appear to be a real person.

[דובר 3] (8:34 - 8:53)
Let's make one candidate together. Isla Sawyer. Let's say I don't like the name.

It's not... Oh, Sophie Wilde. I like the name.

Huh? British. Already she has email, date of birth, everything.

Now I want to put an image set. I'm searching for her pictures. Let's see.

[דובר 4] (8:53 - 9:04)
No, I don't like this man. She doesn't look Canadian, I'm not going to tell you. I have to kill you later and I'm not going to do that.

You know, then we have to trash your body in this land.

[דובר 2] (9:06 - 9:18)
Our investigation pointed to Tim Jorge's work in countries all over the world. Tim Jorge also claimed they can hack Telegram and Gmail.

[דובר 3] (9:19 - 9:24)
This is live, right? We're live at this time. If I write him now, he might answer.

[דובר 4] (9:25 - 9:29)
We can see he finds he's keeping on his drive.

[דובר 2] (9:30 - 10:17)
We don't know exactly how Hanan does the hack, but he claims to be exploiting vulnerabilities in the global signaling system FS7. The Guardian has been leaked emails in which Hanan quotes fees of between $400,000 and $600,000. They also confirm that Tim Jorge worked covertly on the Nigerian presidential election in 2015.

Our investigation has exposed this hidden underworld of disinformation operations. So the next time you're scrolling through your phone or casting a ballot, ask yourself what information you're acting on and who's really behind it.

[דובר 1] (10:39 - 22:12)
That's creepy, right? Since 2015, private disinformation firms like Tim Jorge have allegedly manipulated dozens of elections by deploying bots and fake accounts to spread false narratives at scale. As tech platforms struggle to keep up, these shadowy operations continue to blur the line between authentic public opinion and orchestrated deception.

If shaping public opinion is one aim of political bots, another aim is even more insidious, sowing distrust, fear, and division. By amplifying extreme views, false rumors, and conspiracy theories, inauthentic accounts can pull people apart and erode trust in institutions. As we've seen this semester, Russian trolls' operations around 2016 aim not to promote one view, but to deepen divisions.

The Houston rally is a prime example. Russian-run pages fueled both anti-Muslim hate and fear of Islamophobia to spark conflict. As Senator Mark Warner put it, their goal was to pit Americans against one another.

And it worked. On social media, this often plays as true hashtag campaigns and viral memes engineered to provoke outrage. During the COVID-19 pandemic, for instance, researchers observed that certain conspiracy-laden hashtags were disproportionately driven by bot accounts.

An analysis of 2020-2021 found that Twitter bots were heavily promoting themes like a far-right conspiracy movement QAnon and anti-vaccine or anti-lockdown narratives. Clusters of malicious bots pushed hashtags linking Bill Gates to the pandemic, blaming 5G technology for COVID, or stoking fears that the virus was a hoax or a bioweapon. These messages, often outrageous or blatantly false, nonetheless spread widely thanks to bots, encouraging people to distrust public health authorities and even each other.

Similarly, bots were found amplifying anti-Asian hashtags during the pandemic, trying to inflame ethnic scapegoating. By repeatedly injecting toxic content, fake accounts can make fringe ideas seem popular and lure real users into the fray, each retweet and reply further polarizing the conversation. Bot-driven polarization also plays a major role in nationalistic and sectarian conflicts.

In the Middle East, Saudi Arabia has used bot networks to attack critics and promote its leaders. After journalist Hamal Khashoggi was murdered by Saudi agents in 2018, Twitter bots flooded the platform with praise for the Crown Prince and cast doubt on Saudi involvement. Many accounts, some newly created, amplified official talking points until Twitter suspended them for spam manipulation.

The goal was confusion. If enough posts claim this is a smear by Western media, users may begin to doubt the clear facts. This flood of disinformation erodes trust, leaving people unsure what to believe, damaging democracies and silencing dissent in authoritarian states.

Indeed, polarization itself is often the goal. A revealing statistic from Oxford University's Computational Propaganda Project found that in 48% of countries studied, social media misinformation campaigns were specifically designed to drive division. By reinforcing us-versus-them narratives, left-versus-right, ethnic group-versus-ethnic group, citizen-versus-immigrant, bots act as digital agent provocateurs.

Their target is society's most sensitive fault lines and pure fuel on the fire. What begins as disagreement becomes a toxic scheme of when thousands of fake profiles relentlessly spread extreme content, hateful memes, and conspiracy theories. Over time, exposure to such content hardens attitudes making compromise or empathy increasingly difficult.

This isn't just collateral damage, it's often a deliberative strategy by those to benefit from a polarized, mistrustful public. Perhaps the highest stakes use of political bots is in election campaigns. Since 2020, numerous elections around the world, from presidential races in the Americas to parliamentary contests in Europe, Asia, and Africa, have seen surges of bot activity intended to influence voters.

The objectives vary. Some bots spread disinformation about candidates, some amplify partisan talking points, while others may try to suppress voter turnout by spreading confusion or discouragement. What they have in common is an effort to manipulate democracy's crucial moments through a deceitful online means.

The 2020 U.S. election faced not only heated rhetoric, but also waves of fake accounts spreading misleading narratives. Yale researcher Tawhid Zaman found that during Trump's impeachment, bots made up just 1% of users, but produced 31% of all related tweets, amplifying their influence by sheer volume. These bots weren't shouting into the void.

About 24% of active users followed them, meaning their content reached wide audiences. By flooding feeds with repeated slogans and links, even small bot networks can make fringe ideas seem mainstream. During the campaign, bots pushed false claims about mail-in voting and candidate scandals, a pattern also seen in elections in France, Germany, Indonesia, and Brazil.

A striking example of election meddling via bots occurred in the 2022 Philippine election. Ferdinand Bongbong Marcos Jr., son of former dictator, ran a social media-driven campaign that used micro-influencers, trolls, fake accounts, and bots to whitewash his family's past and smear his opponent, Lenin Obrero. Platforms like Facebook and Twitter were flooded with pro-Marcos content and repeated false claims about Obrero swaying undecided voters.

Though Twitter suspended hundreds of related accounts, much of the damage was done. The campaign's focus on social media over traditional media proved cheaper and more effective, especially among young voters for whom online narratives felt like truth. Oxford's 2020 report found such manipulations in 81 countries, up sharply from earlier years where government agencies involved in 62 of them.

From major powers like Russia and China to small states, bots on social media have become a key tool for swaying elections and shaping public opinion. Mercenary firms like Tim Jorge turned election inference into a paid service, offering hacking and disinformation to the highest bidder. Undercover reports revealed their tactics, including planting fake stories in mainstream media, then amplifying them with bot swarms to manufacture scandals.

In one case, they used bots to flood Twitter with outrage at a UK agency. In another, they staged a fake cyber attack to sway domestic politics. These bots aren't just spammers, they're part of a coordinated influence campaign that blends hacked leaks, fake news and propaganda to sway voters.

A surge of fake praise or viral attacks during a campaign can create the illusion of momentum or scandal, misleading voters and potentially tipping close races. No wonder election monitors now track bots' activity and the polls. Early political bots were often easy to spot.

Obvious usernames, no-profile pictures and awkward, repetitive posts. But the technology and tactics have evolved rapidly. We are now entering a new phase where AI-powered bots can interact with humans in sophisticated, personalized ways.

In a 2023 study of the University of Southern California, researchers created a private social network and deployed chat-GPT-driven social bots with unique personalities and political views. These bots participated in discussions about a fictional political issue alongside real college students who believed they were chatting with other users. The bots didn't just sound human, they adapted their tone, responded in context and held persuasive conversations.

The students were largely unable to detect that they were talking to AI. This marks a major shift. Bots are no longer just broadcasting slogans, they are engaging in dialogue, building fake rapport and influencing opinions in more subtle and interactive ways.

So-called sleeper bots may lay low for months, posting harmless content to build trust, then activate during elections or crises to spread disinformation. Because they blend in so well, they are difficult to detect or remove. Imagine a bot that chats about sports or music, slowly gaining credibility, then suddenly shares a political rumor.

You are far more likely to trust it than to know from a spam account. Some even use AI-generated profile photos and run fake personas across platforms like Facebook, Reddit or YouTube. The result?

Convincing fake people at scale embedded in our digital communities. The honest race between platforms and propagandists is escalating. Social media companies now use AI to detect bot behavior, but as bots become more human-like, tweeting sporadically, gaining followers, simple detection tools fall short.

These bots can manipulate metrics like likes and trending hashtags to create false impressions of consensus or outrage. With low cost and massive scale, even small groups of individuals can run thousands of AI-driven accounts. As a result, our information ecosystem is being weaponized.

What looks like a real person online may be a finely tuned algorithm designed to sway your emotions and opinions. Awareness is our first line of defense. Understanding how bots and fake profiles operate helps us become more critical consumers of information.

When a post sparks outrage or a trend appears suddenly, it's worth asking, who benefits from this and could it be coordinated? While governments and media platforms are cracking down on coordinated inauthentic behavior and journalists and users are improving at spotting fakes, the technology keeps getting better. In our next class, we'll explore what can be done to combat the spread of misinformation.

See you then.
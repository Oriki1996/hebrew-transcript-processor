[דובר 1] (0:09 - 12:37)
You probably do this every day. You open your favorite app, be it Instagram, Facebook, X, or TikTok, you scroll through the news and feeling control, choosing what to read, what to believe, what to share. But beneath that sense of autonomy, invisible systems are shaping our perceptions, exploding your emotions, and steering your actions.

In today's lecture, we'll uncover how social media platforms turn human vulnerability into a weapon and why misinformation spreads faster, further, and deeper than the truth. In the digital age, social media platforms have revolutionized how we communicate, share information, and engage with the world. But alongside these benefits lies a darker reality.

The very architecture of social media is designed in ways that can amplify falsehoods, exploit human vulnerabilities, and even weaponize them. Today's lecture explores those mechanisms through which social media platforms facilitate the rapid and often dangerous spread of misinformation. Let's begin with the concept of network effects.

Social media platforms become more valuable as more people join. This interconnectedness means that a single post, whether factual or false, can be shared, re-shared, and modified at incredible pace. When misinformation enters such a system, it doesn't just remain static, it evolves.

A misleading post on Facebook can be screenshotted, reworded, and reposted on X, TikTok, or Instagram, often with increasing distortions. This creates a cascade effect where misinformation multiplies and spreads like wildfire. A key driver behind this phenomenon is algorithmic amplification.

Most platforms, from YouTube to Instagram, use complex algorithms to determine what content to show each user. These algorithms are optimized for engagement, not accuracy. They analyze your likes, your shares, watch time, and more to predict what you'll interact with next.

Unfortunately, emotionally charged or sensational content, often the hallmark of misinformation, tends to perform best. As a result, false or misleading content is often prioritized simply because it grabs our attention. Over time, this personalization can lead users deeper into the rabbit holes of increasing extreme or conspiratorial content.

A tragic example of algorithmic amplification comes from Myanmar. Starting around 2016, Facebook became the country's main communication platform. At the same time, disinformation targeting the Rohingya, a Muslim minority, spread widely on Facebook, portraying them falsely as terrorists and criminals.

These hateful posts, often pushed by ultranationalist monks and military-linked accounts, were amplified by Facebook's algorithm, which prioritized emotional and sensational content. As fear and anger grew, so did real-world consequences. In 2017, the Myanmar military launched a brutal crackdown against the Rohingya.

They killed around 25,000 people, raped 18,000 women and girls, and forced over 700,000 people to flee the country. A United Nations investigation later concluded that Facebook had played a determining role in fueling the violence. This personalization also contributes to echo chambers formation.

Echo chambers are information environments where people are primarily exposed to content that reinforces their pre-existing beliefs. Algorithms, by design, feed us what we want to see. While this keeps us engaged, it also isolates them from diverse perspectives and critical counter-arguments.

Within these chambers, misinformation is not just accepted, it's amplified, defended, and even normalized. Users may come to believe that fringe views are in mainstream simply because those are the only views they see. Another critical factor is the influence of super-spreaders, a small number of users with large followings, such as influencers, celebrities, or political figures.

When these individuals share misinformation, intentionally or not, their reach and impact are exponentially higher than that of an average user. Studies have shown that a tiny fraction of users are responsible for the majority of false or content going viral. Their authority, popularity, or charisma makes their followers less likely to question the accuracy of what they post.

The design of these platforms also shapes how we interact with content. They influence behavior in subtle but powerful ways. Specific features embedded within social media platforms inadvertently encourage the spread of false or misleading information.

Let's start with the ease of sharing. Social media platforms are engineered to make content sharing as frictionless as possible. With features like the Retweet button on X, Share on Facebook, or Forward on WhatsApp, users can pass along information with just one tap.

This seamlessness turns sharing into an almost automatic reflex, an impulsive action, often driven by an emotional response. Studies show that people are significantly less likely to fact-check or critically evaluate a piece of content before sharing it if the platform makes that process effortless. Another powerful design element is visual modality and ephemerality.

Platforms like Instagram, TikTok, Snapchat, and YouTube prioritize visual content, whether in the form of images, short videos, or live streams. Visual information can be particularly persuasive because of what psychologists call the realism heuristic, our tendency to believe what we see, especially if it looks authentic. A video or photo feels real in a way that text may not.

Furthermore, the ephemeral nature of content, such as disappearing stories or limited-time posts, can reduce the likelihood that users will critically evaluate what they're seeing. If it vanishes in 24 hours, why bother double-checking? Then we have engagement metrics, likes, shares, and comments, which play a central role in how users assess content.

These metrics are not just feedback, they're cues. Content that accumulates a higher number of likes and shares is often perceived as more credible simply because it appears popular. This creates a feedback loop.

High engagement begets more visibility, and more visibility leads to greater trust, even when the underlying information is false. In effect, misinformation that is provocative or emotionally charged is rewarded by the very design of these platforms. Finally, we must consider the difficulty of tracing sources.

On private messaging platforms like WhatsApp and Telegram, messages often arrive without any attribution or context. Who originally wrote it? When and where was it first posted?

In most cases, it's impossible to know. In 2018, Jair Bolsonaro won the Brazilian presidential election partially by spreading lies against his opponents on WhatsApp. His supporters posted that left-wing candidate Fernando Haddad was disturbing gay kids in schools, which encouraged kids as young as six to become gay.

The anonymity of these messages made accountability nearly impossible, and severely hampered fact-checking efforts. Even when misinformation is detected, identifying and correcting the original source can be like chasing shadows. Up until now, we've examined how the design of social media platforms and their algorithms can unintentionally facilitate the spread of misinformation.

But in this segment, we turn to something more deliberate and disturbing, the intentional exploitation of human vulnerabilities. Social media is not just a passive environment for misinformation, but an active battleground where our psychological and emotional weaknesses are weaponized. Let's begin with intentional disinformation and manipulation.

Not all misinformation is spread by accident. Increasingly, malicious actors, ranging from state-sponsored agencies and political operatives to ideologically driven individuals, create and spread false information for strategic purposes. Their goals vary, to influence election, sow chaos, undermine public trust, or deepen social divisions.

These actors don't just throw random lies into the wind. They craft their messages carefully, often using emotionally resonant language and targeting groups that are already predisposed to believe the narrative. A fake news story about election fraud, for example, is more likely to be believed and shared in communities already primed to distrust electoral institutions.

Next, consider the radicalization pipelines that some platforms inadvertently create. Algorithms designed to maximize user engagement can end up pushing users towards increasingly extreme content. Just like in this cartoon, on platforms like YouTube or TikTok, watching a video on vaccine skepticism might lead you to suggestions for videos about government conspiracies or anti-science movements.

The logic is simple. If a user clicks on an emotionally charged or controversial content, they are likely to click again. So the platform feeds them more of the same, only more extreme.

Over time, this creates a feedback loop in which users are drawn deeper into radical ideologies. What starts as curiosity can become belief and eventually identity. That's why some scholars refer to this pipeline not just a path.

A third mechanism of exploitation is targeting and polarization. Social media platforms allow advertisers, and by extension disinformation agents, to segment users based on detailed behavioral data. What they like, what they fear, what makes them angry.

This enables highly specific micro-targeting. Political operatives can, for instance, send fear-mongering messages about immigration to one demographic, while sending economic anxiety messages to another demographic. These tailored messages often contradict one another, but are very effective at precisely because they exploit each group's unique vulnerabilities.

The result is not just persuasion, but polarization. People see different realities based on the content they are shown, leading to deeper division and mutual distrust. In short, social media platforms do not merely expose us to disinformation, they are engineered in ways that allow it to thrive.

Their algorithms amplify emotionally charged content, their design encourages impulsive sharing, and their business model rewards engagement above all else. As long as false or misleading content keeps users scrolling, clicking, and reacting, it remains profitable. And so, the very vulnerabilities that make humans become commodities in a system where attention is currency and truth is optional.

As you'll hear in the next segment, social media has become an agent of chaos. I hope you find it interesting. See you next time.
[דובר 1] (0:09 - 3:00)
Imagine a frantic call, your daughter is sobbing, mom, I messed up. You instantly recognize her voice. Then a stranger's harsh tone claims kidnapping.

She cries for help. Panic sets in as you envision her horrific threat. But what if she's a lie?

What if this is a lie? What if your daughter was never in danger, never on the phone? The voice, her voice, was an AI-generated deepfake, a perfect auditory illusion.

This chilling scenario became a reality for Jennifer DiStafano, who testified that the call sounded exactly like her daughter's voice, down to the sobs. It was a cruel hoax, a digital manipulation, living real terror. As a parent and someone who cares deeply about truth, I find this deeply unsettling.

We are hardwired to trust our senses. A child's cry, a familiar face, these trigger not just data processing, but a rush of emotions, memories, and urgent actions. DiStafano's experience highlights how deepfake voice cloning can exploit these instincts, creating an auditory and mirage-bypassing rational filters.

By the time she knew it was fake, the physiological panic had already taken hold. This is the human cost. Deepfakes prey on the fundamental trust we place in our own sight and hearing.

In this lecture, I want to discuss how deepfakes, both audio and video, disrupt our brain's ability to trust our senses and its implications. We'll focus on the human side, the psychology, neuroscience, and personal stories in a world where seeing is no longer believing. From birth, our brains learn whom to trust by sight and sound.

Newborns recognize their mother's voice at birth, even before visual recognition develops months later. Evolutionary voice and lifeline in the womb and early life exist. A caregiver's voice suits, and then a baby's brain associates it with safety and love.

Soon, faces are learned. By three months, infants reliably recognize their mother's face, especially with combined voice cues. Our eyes and ears become the primary channels of trust.

Normally, sight and sound reinforce each other. Seeing a friend and hearing their voice creates a double handshake of identity, confirming, yes, this is truly them. Psychology calls this multisensory integration.

The McGurk effect dramatically illustrates this. A video of a fa with an audio of ba often results in hearing a blend of confusion. Please watch this example.

[דובר 2] (3:18 - 4:05)
Did it change again? This is called the McGurk effect. It's an illusion that shows how visual cues can have an impact on our perception of speech.

In these clips, the speaker is only saying ba, but when you see his mouth make the sound fa, you hear that instead, even though the sound he's making hasn't changed. If you focus on the left side of the screen, you probably hear ba. If you focus on the right side, you probably hear fa.

But the only sound playing is ba. The McGurk effect explains this phenomenon. The visual information you receive by watching the mouth move affects your perception of the auditory information you receive by listening to the sound.

Let's play it again.

[דובר 3] (4:06 - 4:14)
ba, ba, ba, ba, ba, ba.

[דובר 1] (4:14 - 14:55)
Are you experiencing the McGurk effect? As you saw, the brain struggles to reconcile conflicting sensory information, sometimes even hallucinating a third sound. This shows how interwined our sight and hearing are in decoding information.

We aren't used to them disagreeing. When they do, our minds struggle to choose. Harmony between sight and sound usually boosts our confidence in what we perceive.

Inside our brains, dedicated networks recognize familiar voices and faces. The fusiform face area, part of the right fusiform gyrus, specializes in facial recognition. Seeing a face, especially a known one, activates it, encoding unique features and matching them to stored memories.

A match, be it grandma or my roommate, connects to the emotional memory of that person. I trust grandma or I like my roommate. Similarly, voice-selective regions in the auditory cortex distinguish familiar voices by pitch and cadence.

Our brains have built-in color ID for voices and facial ID for faces, operating instantly and unconsciously. Let's take a moment to appreciate just how automatically and reliably our brains recognize familiar faces. It begins with structural encoding, when our visual system processes the basic features of a face, eyes, nose, mouth, and their spatial arrangement.

This information is then compared against face recognition units, or FRUs, which act like a mental Rolodex of stored face templates. If a match is found, the brain accesses personal identity nodes, or PINs, the biographical and emotional information tied to that person, such as, she looks like Taylor Swift. Finally, the process culminates in name retrieval, where the person's name and associated memories surface to consciousness.

Yes, that's Taylor Swift, the great country singer who also has some cute pop songs. That's what's in my brain. This entire sequence unfolds in milliseconds.

When it works, it feels seamless, and trust is a natural byproduct. Recognizing someone doesn't just identify them, it also triggers the emotional history you share. The process of hearing a familiar voice is remarkably similar.

Your brain decodes the sound, matches it to stored voices' memories, identifies the speaker, and activates the emotional context that comes with them. This ability is fundamental to everyday life. Every video call and every phone conversation depends on our brain's effortless processing of voices and faces.

We trust these signals because, until now, they've been overwhelmingly reliable. While pranks and impersonations have always existed, never before has it been possible for anyone with basic skills and a computer to perfectly fake both a voice and a face. Deepfakes change this, challenging the brain's core assumption that seeing is believing.

So what is a deepfake? It's a convincingly fabricated audio or video using artificial intelligence, typically generating adversarial networks, or GANs. These networks learn from real images and recordings and produce a new fake content that mimics the source.

A deepfake video might swap faces, making it appear someone said or did something they didn't, while a deepfake audio clones the voice tones and patterns to create fabricated speech. The result is a sensory counterfeit that feels real to our eyes and ears, even if our rational minds suspect otherwise. Deepfakes gained notoriety around 2017 and 2018 through the non-consensual use of AI to paste celebrity faces into pornography.

The technology has since rapidly advanced into various domains, including deepfake celebrity cameos, oaks, and face swaps for entertainment. The deep Tom Cruise TikTok account, featuring a Tom Cruise lookalike, performing playful acts astonished viewers worldwide. Was it the real movie star goofing off?

It was a deepfake by visual effects artist Chris Hume using an actor and digitally grafting Cruise's face. The impression was striking, and due to the actor's mannerism and high visual fidelity, it worked. Only close inspection revealed tiny glitches.

Millions of casual viewers were fooled, even briefly. Our brain's eagerness to believe what looks familiar helps explain why deepfakes like the Tom Cruise videos are so convincing. This instinctive acceptance ties directly into a phenomenon known as the uncanny valley, the unease we feel when something artificial looks almost human but not quite perfect.

Slight imperfections trigger subconscious alarms bells. The amygdala, the brain's fear center, might activate. Ironically, high-quality deepfakes are starting to climb out of this valley.

They are becoming so good, they don't always trigger creepiness. Instead, they trigger recognition. We see the expected person, and our brain's facial recognition says, yeah, that's him, while our emotional brain responds as if it would be a real person, bypassing the usual alarm.

A successful deepfake can short-circuit our protective emotional responses, presenting such a clean fake that the discrepancy isn't detected, like a perfect forgery accepted by an expert. Neuroscience research supports this. A 2024 study using fMRI showed that while participants consciously detected only 2 3rd of deepfake voices, their brain showed different activity patterns when processing real versus fake voices.

A corticostreatal network involving the auditory cortex and the right nucleus accumbens, which is linked to reward and novelty, activated distinctively for fake voices. This suggests a neural early warning system for audio forgeries, picking up solid inconsistencies even when we don't consciously notice. However, this doesn't always translate to conscious detection without training.

Still, this inherent ability offers some hope for sharpening our defenses for the future. Video deepfakes pose a similar challenge, exploiting our brain's identity verification mechanisms. High-quality deepfakes feed erroneous data to the fusiform face area in the facial recognition process.

If well done, our brain readily says, that's Taylor, even if it's not. If less perfect, we might sense something is off, but not immediately think, hmm, deepfake, perhaps attributing it to a bad video quality or our own tiredness. The net effect is a serious erosion of our usual certainty in recognizing people, and certainty is the foundation of trust.

What happens to trust when our senses can be so easily fooled? The consequences go far beyond embarrassment. They affect relationship and social stability.

Imagine receiving a voice message from your best friend, urgently asking for money. The voice is perfect, but the request feels off. Is it real or a hijacked voice?

In 2019, criminals cloned a CEO's voice and convinced an executive to transfer $243,000, mimicking even the CEO's accent. Trust, as this case shows, is not just emotional, it's transactional. When deception is this seemingless, we face a painful dilemma.

Follow our own instincts or risk ignoring a real plea for help. Conversely, consider the boy who cried wolf problem, or what scholars call the liar's dividend. As deepfakes become more common, those with something to hide can exploit the technology's existence to deny reality.

A compromising video or incriminating audio can simply be dismissed as AI generating, casting doubts even when it's authentic. Take the case of the 2005 Access Hollywood recording where Donald Trump was caught on tape making obscene remarks about women. While he initially acknowledged it, he later suggested the tape might be fake.

This shift exemplifies the liar's dividend in action. Even real evidence can be undermined by the mere suggestion of artificial manipulation. We're entering an era where our default trust in recorded evidence is eroding, giving liars a powerful new tool to escape accountability.

Furthermore, even genuine content can lose its impact. Constant second-guessing can lead to reality apathy, making us jaded and less emotionally responsive. If real evidence of wrongdoing is dismissed as a deepfake, victims might not get justice or empathy.

Society could become more cynical and less empathetic simply because we don't know what to believe. To sum up, deepfakes may be devastating on a personal level. In South Korea, for example, non-consensual deepfake pornography has targeted thousands of women, leading to public shaming, harassment, and long-term psychological trauma.

These forgeries can destroy reputations, relationships, and even mental health. But while the arm to individuals is undeniable, the stakes become even higher when deepfakes enter the political arena. When synthetic media is used not just to humiliate one person, but to manipulate entire electorates, distort public debates, or incite conflict or war, the damage scales exponentially.

As we'll see in the next lecture, political deepfakes have the potential not just to ruin individual lives, but to destabilize whole nations and affect the lives of millions.
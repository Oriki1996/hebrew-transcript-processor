[דובר 1] (0:09 - 6:11)
On October 7th, 2023, as sirens wailed and gunfire echoed in southern Israel, another war was already unfolding, online. While Israelis were still reeling from the shock of the Hamas attack, a surge of coordinated disinformation flooded social media. Thousands of fake accounts, powered by global bot farms linked to Russia and Iran, began pushing a chilling narrative that the attack was staged by Israel itself.

Phrases like, traitors in the army and don't trust your commanders were shared and re-shared at scale, not by angry citizens, but by racks of smart force humming quietly in distant warehouses. The goal wasn't just to confuse, it was to erode trust from within, to turn fear into doubt and doubt into collapse. This wasn't just a battle for territory, it was a battle for belief.

Welcome to the age of automated mind control. On social media, fake speech amplified by bots drown out real thought. It's not just that the lies are being told, it's that they are repeated by thousands until they feel true.

A single lie is forgettable, but a lie echoed by an army of bots, that's how belief is hijacked. What exactly are bots and bot farms? A bot is a fake user account, often automated, that mimics real people online.

A bot farm takes this further, hundreds or thousands of real smartphones, often controlled by a single computer, working in sync. These devices run fake accounts to like, comment and share posts in a coordinated way. Bots are now everywhere online.

By 2023, nearly half of all the internet traffic came from bots, not humans. Some are helpful, like search engine crawlers or voice assistants like Alexa and Google Assistant, which experts predict will reach this year 8.4 billion devices, outnumbering the people on earth. Bots handle everything from online searches to consumer pro-support chats, and they're increasingly woven into our everyday routines.

Bots may seem like a modern invention, but they've been around for decades. The first chatbot, Eliza, was created in 1966 at MIT. Though primitive, it mimicked a therapist well enough to convince users they were talking to someone who understood them.

That illusion, the Eliza effect, was an early sign of our tendency as humans to see a mind behind the machine. Modern bots have evolved from such basic scripts to complex conversational agents powered by AI. Tools like ChatGPT, Siri and Alexa use neural networks trained on billions of human interactions to generate real-time responses.

Bots have gone from parrots to predictive learners. But the same qualities that make bots useful—speed, scalability, and realism—also make them dangerous. Bot farms now exploit social media platforms to manipulate attention, sentiment, and belief.

The goal? To trick algorithms into thinking a post is trending. This tactic, called by Meta coordinated inauthentic behavior, makes content seem popular even when no real people are involved.

It might be to stalk tips, it may be about celebrity drama or political propaganda. Either way, the buzz is fake. Distorting public perception isn't new.

But in the past, it used to take a bit more effort. For instance, in 1929, President Kennedy's father, Joseph P. Kennedy, got rich by manipulating RCA's talks through secret deals and media hype.

He and his wealthy friends pumped the price, then dumped their shares before the crash, leaving regular investors in ruins. Today's manipulators use bot farms instead of trading pools. Fake accounts flood Reddit, Discord, and X with hype like, buy the dip and going to the moon, tricking algorithms and real users into chasing worthless stocks.

There's often no news, no economic markers, just bots posting things like, this company is going up or best deal on the market. But none of it is based on real economic indicators, like good profits or new market opportunities. In a world where all information is now suspect and decisions are based on sentiment, bot farms amplification had democratized market manipulation.

But stock trading is only one application. Anyone can use bot farms to influence how we invest, make purchasing decisions, or even vote. These strategies, which originated in propaganda efforts pioneered by Russia and Iran to broadcast beheadings and sway elections, have now been honed to sell stocks, incite riots, and tarnish celebrity reputations.

Bot farms thrive in countries where phone numbers, electricity, and labor are cheap, and government oversight is minimal. Places like Thailand, Indonesia, India, and Egypt. These farms charge around a penny or less per action to like posts, to follow accounts, to leave a comment, to watch videos, and visit websites.

Sometimes positioned as growth marketing services, they sell on gig marketplaces or direct to consumer websites. Here, take a look at this clip.

[דובר 2] (6:12 - 6:37)
From Instagram accounts with thousands of likes and followers, to Facebook posts that go viral overnight. Ever wonder if it's actually legit and what could be behind those crazy spikes? Enter the world of click farms.

Undercover operations, where actual people are paid to monitor hundreds of phones, clicking on posts, following accounts, liking photos, and watching videos, all to drive up fake traffic.

[דובר 3] (6:37 - 6:46)
This is really easy. I mean, this is unskilled labor. You could train a monkey to do this.

And then as far as the cost, I mean, you're paying people pennies on the dollar to perform these tasks.

[דובר 2] (6:46 - 7:12)
So just how easy is it? We decided to create our own click farm with the help of cyber security expert Jim Stickley and 50 cell phones creating Instagram accounts for each one ready to click on whatever we wanted. And that's where I come in.

Let's say I wanted to get a bunch of views on this really boring video of me watering plants that I put on this account that I made specifically for this reason. I'm gonna put this right here and we're gonna watch this on all these phones, right?

[דובר 3] (7:12 - 7:18)
Correct, we've already got it all queued up, all gonna be ready to go. And more importantly, right here, we can watch and see how many views we get.

[דובר 2] (7:18 - 7:45)
All right, let's get some views. I get clicking. That is some rich content.

I see the same video play out over and over. I'm watering plants. In minutes, 11 views.

The views start coming in. And while we're here, we might as well get some followers too. I follow that fake account from every single phone, easily hitting 50 followers.

Nothing to see here. Then passing 50 views. And I am just one guy with one wall.

[דובר 3] (7:45 - 7:59)
How big of an operation can these get? Way bigger. The average person working with these ClickFarms will be one person for 200 phones.

And a ClickFarm might have hundreds if not thousands of employees clicking at these things all day long. We're not done yet.

[דובר 2] (7:59 - 8:58)
I want more views. Find Instagram views. It turns out it is way too easy to get them online.

A simple Google search shows me tons of sites that can get me instant views for cheap. All right, so this one has five stars. I'm going to click in here and oh, wow.

10,000 views for 24 bucks. And they take credit card. Just a few seconds later.

And we've got 63 views. The numbers keep climbing. 7,731.

Skyrocketing well over 10,000. All right, so we only bought 10,000 views, but the latest number is 16,481. It looks like this video is going viral.

But remember, this is all fake. And Instagram isn't the only one. You can buy friends and followers and views and likes on almost any social media platform out there.

Instagram, Facebook, Twitter, and YouTube telling NBC News this type of activity is not allowed. And they're continuing to develop systems to prevent, detect, and remove accounts that violate their policies. What's the harm in this?

Is this fraud?

[דובר 3] (8:58 - 9:06)
It's absolutely fraud. And you think about it from the standpoint of just a consumer. You're being misled.

So something goes viral. And you go to look at it, but it's not real.

[דובר 1] (9:07 - 16:43)
Crazy how easy and cheap it is, right? While much of their activity is commercial, boosting interest in products or celebrities, they're increasingly used to manipulate politics, protest, and even war. Unfortunately, young people don't go to Google anymore.

They go to TikTok and Instagram and search for questions they want answered there. It requires zero critical thinking, but somehow feels more authentic. Research shows that users feel like they are making their own decisions, but they're actually being fed propaganda tailored to skew their perspective.

It's not an easy problem to solve. And social media companies appear to be buckling under the weight of inauthenticity. After Elon Musk took over Twitter, now X, the company fired much of its anti-misinformation team and reduced platform transparency.

Meta is phasing out third-party fact-checking. YouTube has rolled back features meant to combat misinformation. If there's no trustworthy information, what we think becomes less important than how we feel.

That's why we're regressing from an age of science where critical thinking and evidence-based reasonings were central back to something driven by emotional reasoning and algorithmic manipulation. When Twitter launched, it felt like a breakthrough, a tool to amplify knowledge and empower activism. In its early days, it helped coordinate protests during the Arab Spring, from Cairo's Tahrir Square to the streets of Tunis, giving voices to movements that toppled regimes.

No one imagined it would become a weapon. Its founders saw a platform for connection and change, but they didn't anticipate how easily it could be hijacked by bad actors and malicious intent. Today, what gets the most clicks gets the most reach, and outreach is the fastest way to go viral.

Social media manipulation is no longer amateur. It's strategic, sophisticated, and often state-sponsored. Malicious actors engineer virality by embedding bots into communities months or even years before they activate.

Bots are given realistic profile pictures, bios, and language settings to blend in. They use tricks like aligning post times to local time zones and adjusting smartphone clocks and device fingerprints to seem authentic. With AI-generated personas, bots follow real users, comment on content, and cross-engage with other bots to build credibility.

This tactic, social graph engineering, targets communities that lean politically left or right, infiltrating interest groups like the BTS Army, the global fan base of the South Korean band BOYS! BTS, or raising forums, gaining trust before disrupting from within. To avoid detection and deletion, early bots had to pose as real people, stealing profile photos, mimicking human behavior, and occasionally posting harmless updates like going to a restaurant or attending a game.

It was clunky, costly, and easy to spot. But today, with AI-generated images advancing at lightning speed, bots can create entirely fake personas in seconds for free. The images look real enough to fool even both humans and algorithms.

In the cat and mouse game of online deception, the mice are winning. Bot handlers now use platforms like ChatGPT, Gemini, or Cloud, paired with visual tools like Make.com, to build advanced flows. These bots aren't just copying and pasting.

They're generating personalized on-topic posts that sound like real people, from a liberal retiree to a conservative mechanic. These evolutions make manipulations more believable and more dangerous. All the bots were simple.

They spammed identical text blocks, what we call copypasta, to flood conversations. But modern bots are smarter. They reshare trending content using keywords that personalize their home feed, helping them stay on topic and blend into communities.

Frequent platform updates also help make bot activity look fresh and spontaneous. Bots aren't always malicious. In Rwanda, for instance, a digital health bot called Babbel offers over 2 million citizens a virtual doctor consultation via text.

This is especially valuable where no human doctors are in place. The UK's National Health Services uses similar bots to answer medical queries and ease pressure on clinics. You've probably chatted with a bot yourself, on a bank's website, or while rescheduling a flight.

Over two-thirds of consumers globally have used a chat bot for support, and more than a third of businesses now rely on them. They're fast, they're friendly, and they're always on. But as bots become more natural and harder to distinguish from real users, the risk of deception grows.

These bot farms go beyond stocks and celebrity gossip. Today, they're powerful tools for weaponizing narrative. If we go back to the war in Gaza, alongside organic pro-Palestinian content, Russian and Iranian bots have amplified posts, framing the conflict as a simple civil rights struggle, erasing Hamas' fundamentalist agenda and casting the issue in a victim-colonizer binary.

The aim was not to inform, it was to inflame and divide democratic societies. This manipulation has real-world impact. A Naudi Bank claim that Israel bombed a Gaza hospital, initially boosted by bots and shared widely, illustrates how fast false narratives take hold.

Even after retractions, the damage lingers. As bot detection firm Expos reports, coordinated bot networks linked to Russia, Iran and China have amplified both anti-Semitic posts and anti-Ukraine propaganda, adapting messaging to fit different audiences. Meanwhile, Israel itself ran covert influence campaigns targeting US lawmakers during the war, using fake accounts to maintain America's military support.

On the other hand, on college campuses, anti-Israel protests erupted within hours of the attack before the scale of the massacre was even clear. In a bot-driven world, engagement metrics no longer reflect truth. They reflect whoever shouts the loudest or programs the best.

Now, as we reach the end of this lecture, I have a confession to make. This lecture was not written by me. It's actually a carefully engineered blend of a Herrick Schwarzman's article, Bot Farms Invade Social Media to Hijack Popular Sentiment, and suggestions I got from Chad GPT, a bot.

Yes, Chad GPT had a hand or algorithm in shaping this talk you just listened to. In a few years, you may not be able to tell who wrote what anyway. So if you liked it, I'll take the credit.

If you didn't, blame it on the bot.